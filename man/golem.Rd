% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/golem.R
\name{golem}
\alias{golem}
\title{Regularized Generalized Linear Models}
\usage{
golem(family = c("gaussian", "binomial"), penalty = c("slope",
  "group_slope", "lasso"), solver = "fista", intercept = TRUE,
  standardize_features = TRUE, orthogonalize = TRUE,
  sigma = c("sequence", "estimate"), n_sigma = 100,
  sigma_min_ratio = NULL, lambda = NULL, n_lambda = 100,
  lambda_min_ratio = NULL, fdr = 0.2, tol_rel_gap = 1e-06,
  tol_infeas = 1e-06, max_passes = 10000, diagnostics = FALSE)
}
\arguments{
\item{family}{response type. See \strong{Families} for details.}

\item{penalty}{the regularization penalty to use. See \strong{Penalties} for
details.}

\item{solver}{the numerical solver to use. See \strong{Solvers} for details.}

\item{intercept}{whether to fit an intercept}

\item{standardize_features}{whether to standardize features (predictors)}

\item{orthogonalize}{whether \code{x} should be orthogonalized. Note that
setting this to TRUE when \code{x} is sparse will through an error. (only
applies to Group SLOPE)}

\item{sigma}{noise estimate (only applies to SLOPE and Group SLOPE)}

\item{n_sigma}{number of sigmas to generate (only relevant for
SLOPE and Group SLOPE)}

\item{sigma_min_ratio}{smallest value for \code{sigma} as a fraction of}

\item{lambda}{either a character vector indicating the method used
to construct the lambda path or}

\item{n_lambda}{length of regularization path (only relevant for lasso)}

\item{lambda_min_ratio}{smallest value for \code{lambda} as a fraction of
\eqn{\lambda_\mathrm{max}}{\lambda_max} (only applies to lasso)}

\item{fdr}{target false discovery rate (only applies to SLOPE and
Group SLOPE)}

\item{tol_rel_gap}{relative tolerance threshold for duality gap check}

\item{tol_infeas}{tolerance threshold for infeasibility}

\item{max_passes}{maximum number of passes for optimizer}

\item{diagnostics}{should diagnostics be saved for the model fit (timings,
primal and dual objectives, and infeasibility)}
}
\value{
An object of class \code{"Golem"}.
}
\description{
This functions fits a generalized linear model (GLM) using efficient
optimization routines suitable to big data problems.
}
\details{
The objective for each model is simply the loss function for
each family plus a penalty term.
}
\section{Methods}{


\describe{
\item{\code{fit(x, y, groups = NULL, warm_start = TRUE, ...)}}{
This method fits models specified by \code{\link[=golem]{golem()}}.
\describe{
\item{\code{x}}{
the feature matrix, which can be either a dense
matrix of the standard \emph{matrix} class, or a sparse matrix
inheriting from \link[Matrix:sparseMatrix]{Matrix::sparseMatrix} Data frames will
be converted to matrices internally.
}
\item{\code{y}}{
the response. For Gaussian models, this must be numeric; for
binomial models, it can be a factor.
}
\item{\code{groups}}{
a vector of integers giving the group membership of each
feature (only applies to Group SLOPE)
}
\item{\code{warm_start}}{
whether to use the coefficient estimates from the previous
fit when refitting the model using new data. Provided that
the same penalty (with approximately the same parameters)
is used, setting this to true might lead to
substantial performance boosts.
}
\item{\code{\dots}}{
arguments that will be used to modify the original model
specification from the call to \code{\link[=golem]{golem()}}. Note that arguments
pushed through this interface will modify the model, which
might have unintended consequences.
}
}
}
\item{\code{coef()}}{
Return the coefficients from the model fit (after dropping extraneous
dimensions). If you prefer to always return a three-dimensional array,
call \code{model$coefficients} instead.
}
\item{\code{predict(x, type = c("link", "response", "class"))}}{
Return predictions from models fit by \code{\link[=golem]{golem()}} based on
new data.
\describe{
\item{\code{x}}{new data to make predictions for.}
\item{\code{type}}{
type of predictions to make. \code{"link"} gives the linear
predictors, \code{"response"} gives the result of applying the link
function, and \code{"class"} gives class predictions.
}
}
}
\item{\code{plot(...)}}{
Plot the model's coefficient along the regularization path.
\describe{
\item{\code{\dots}}{
graphical parameters for the plot passed on
to \code{\link[lattice:xyplot]{lattice::xyplot()}}.
}
}
}
}
}

\section{Families}{


\strong{Gaussian}

The Gaussian model (Ordinary Least Squares) minimizes the following
objective.

\deqn{
  ||\boldsymbol{y} - \boldsymbol{X\beta}||_2^2
}{
  ||y - X\beta||_2^2
}

\strong{Binomial}

The binomial model (logistic regression) has the following objective.

\deqn{
  \sum_{i=1}^n \log\left[1+ \exp\left(- y_i\boldsymbol{x}_i'\boldsymbol{\beta} \right) \right]
}{
  \sum log[1+ exp(- y_i x_i' \beta)]
}
}

\section{Penalties}{

Models fit by \code{\link[golem:golem]{golem::golem()}} can be regularized via several
penalties.

\strong{Lasso}

The Lasso penalizes coefficients using the L1 norm. The penalty
term in the lagrangian form of the loss function is

\deqn{
  \lambda \sum_{j=1}^p |\beta_j|
}{
  \lambda \sum |\beta|
}

\strong{SLOPE}

SLOPE (Sorted L-One Penalized Estimation) is an extension of the Lasso.
Unlike the latter, however, SLOPE uses a non-increasing
sequence of \eqn{\lambda}---one
for each coefficient. The penalty term looks like

\deqn{
  \sigma \sum_{i=j}^p \lambda_j |\beta|_{(j)}
}{
  \sigma \sum \lambda |\beta|(j)
}

\strong{Group SLOPE}

Group SLOPE is an extension of Group LASSO. It applies the following
penalty

\deqn{
  J_\lambda(\beta) = \sum_{j=1}^p \lambda_j |W||\beta||_{I,X}|_{(j)}
}{
  J_\lambda(\beta) = \sum \lambda_j |W||\beta||_{I,X}|_(j)
}
}

\section{Solvers}{

There is currently a single solver available for \link[golem:golem]{golem::golem}.

\strong{FISTA}

FISTA (Fast Iterative Shrinking-Tresholding Algorithm) is an extension
of the classical gradient algorithm.
}

\examples{

# Gaussian response, slope penalty (default) --------------------------------

# Specify the model
gaussian_model <- golem(family = "gaussian")

# Fit the model
gaussian_model$fit(abalone$x, abalone$y)

# Get coefficients from the model fit
gaussian_model$coef()

# Binomial response, lasso penalty ------------------------------------------

binomial_model <- golem(family = "binomial", penalty = "lasso")

binomial_model$fit(heart$x, heart$y)

}
