---
title: "Regularization penalties in golem"
author: "Johan Larsson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: golem.bib
vignette: >
  %\VignetteIndexEntry{Regularization penalties in golem}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Sorted L-One Penalized Estimation (SLOPE)

The SLOPE penalty attempts to control
the false discovery rate (FDR) of non-zero coefficients (feature weights)
in the model. It is in many ways analogous to the
Benjamini--Hochberg procedure for multiple comparisons.

Let's set up a simple experiment to see how SLOPE controls the FDR.

```{r, fig.cap = "Control of false discovery rate using SLOPE."}
library(golem)
library(lattice)
library(latticeExtra)

# proportion of real signals
q <- seq(0.05, 0.5, length.out = 20)
fdr <- double(length(q))
set.seed(1)

for (i in seq_along(q)) {
  n <- 1000
  p <- n/2
  k <- floor(q[i]*p)
  sigma <- 1
  problem <- golem:::random_problem(n, p, k, sigma = sigma)
  
  x <- problem$x
  y <- problem$y
  signals <- problem$nonzero
  
  fit <- golem(x, y, penalty = "slope", lambda = "bhq", sigma = 1,
               intercept = FALSE)
  
  selected_golem <- which(coef(fit) > 0)
  V <- length(setdiff(selected_golem, signals))
  R <- length(selected_golem)
  fdr[i] <- V/R
}

xyplot(fdr ~ q, type = "b", ylab = "FDR") +
 layer_(panel.refline(h = 0.2)) 
```

## Group SLOPE

The Group SLOPE penalty [@brzyski2018] works very much like the
regular SLOPE penalty but on the group level instead of 
on a per-observation basis. 

Much like the group lasso, Group SLOPE ensures that the coefficients
for the features of a given group are either all zero or all non-zero.

## References
