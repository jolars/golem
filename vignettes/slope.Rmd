---
title: "The SLOPE Penalty"
author: "Johan Larsson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The SLOPE Penalty}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Sorted L-One Penalized Estimation (SLOPE)

The SLOPE penalty attempts to control
the false discovery rate (FDR) of non-zero coefficients (feature weights)
in the model. It is in many ways analogous to the
Benjamini--Hochberg procedure for multiple comparisons.

Let's set up a simple experiment to see how SLOPE controls the FDR.

```{r, fig.cap = "Control of false discovery rate using SLOPE."}
library(golem)
library(SLOPE)

# proportion of real signals
q <- seq(0.05, 0.5, length.out = 20)
fdr <- double(length(q))
set.seed(1)

for (i in seq_along(q)) {
  n <- 1000
  p <- n/2
  k <- floor(q[i]*p)
  sigma <- 1
  problem <- golem:::random_problem(n, p, k, sigma = sigma)
  
  X <- problem$X
  y <- problem$y
  signals <- problem$nonzero
  
  fit <- golem(X, y, penalty = slope(lambda = "bhq", sigma = 1),
               intercept = FALSE)
  
  selected_golem <- which(coef(fit) > 0)
  # selected_slope <- slope_fit$selected
  #length(selected_golem)/length(selected_golem)
  V <- length(setdiff(selected_golem, signals))
  R <- length(selected_golem)
  fdr[i] <- V/R
}

library(lattice)
xyplot(fdr ~ q, type = "b",
       panel = function(...) {
         panel.refline(h = 0.2, lty = 2, col = "dark grey")
         panel.xyplot(...)
       })
```



